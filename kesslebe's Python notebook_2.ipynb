{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-default-python-general_8gb_2cpu",
      "display_name": "Python in general_8GB_2CPU (env default-python)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "customFields": {},
    "createdOn": 1761590621793,
    "modifiedBy": "kesslebe",
    "tags": [],
    "creator": "kesslebe"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport openai\nimport logging\nimport dataiku\nimport resource\nimport pandas as pd\nimport threading\nimport re\nimport unicodedata\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport subprocess\nimport os\nfrom time import time\nfrom utils_prompt import prepare_excel_for_pdf, convert_office_to_pdf_optimized, extract_image_pdf\n\n# Set OpenAI parameters\nlogging.getLogger(\u0027openai\u0027).setLevel(logging.WARNING)\nlogging.getLogger(\u0027httpx\u0027).setLevel(logging.WARNING)\nclient \u003d dataiku.api_client()\nauth_info \u003d client.get_auth_info(with_secrets\u003dTrue)\nfor secret in auth_info[\u0027secrets\u0027]:\n    if secret[\u0027key\u0027] \u003d\u003d \u0027neura-gpt-test-key\u0027:\n        API_KEY \u003d secret[\u0027value\u0027]\n        break\n        \nopenai.api_type \u003d \u0027azure\u0027\nopenai.azure_endpoint \u003d \u0027https://iapi-test.merck.com/gpt/libsupport\u0027\nopenai.api_version \u003d \u00272025-03-01-preview\u0027\nopenai.api_key \u003d API_KEY\nmodel_name \u003d \u0027gpt-5-2025-08-07\u0027\nprice_per_mil_token \u003d (1.25, 10)\n\n# Read recipe inputs\nfolder_pref \u003d \u0027/tmp/auto_soe\u0027\nenviron \u003d os.environ.copy()\nenviron[\u0027XDG_RUNTIME_DIR\u0027] \u003d folder_pref\nos.makedirs(folder_pref, exist_ok\u003dTrue)\n\nfolder \u003d dataiku.Folder(\u0027Bi5hoVjN\u0027)\nall_paths \u003d folder.list_paths_in_partition()\n\n# Initialize datasets\noutput_data \u003d dataiku.Dataset(\"QD11C\")\nlog_data \u003d dataiku.Dataset(\"QD11L\")\n\n# Define the schemas explicitly\noutput_schema \u003d [\n    {\"name\": \"Dir_Name\", \"type\": \"string\"},\n    {\"name\": \"File_Name\", \"type\": \"string\"}, \n    {\"name\": \"Chunk_Id\", \"type\": \"int\"},\n    {\"name\": \"Chunk_Text\", \"type\": \"string\"},\n    {\"name\": \"Chunk_Length\", \"type\": \"int\"},\n    {\"name\": \"Token_Count\", \"type\": \"int\"}\n]\n\nlog_schema \u003d [\n    {\"name\": \"directory\", \"type\": \"string\"},\n    {\"name\": \"file_name\", \"type\": \"string\"},\n    {\"name\": \"file_size\", \"type\": \"string\"},\n    {\"name\": \"memory_total\", \"type\": \"string\"},\n    {\"name\": \"page_count\", \"type\": \"string\"},\n    {\"name\": \"token_count\", \"type\": \"string\"},\n    {\"name\": \"extract_duration\", \"type\": \"string\"},\n    {\"name\": \"price_total\", \"type\": \"string\"},\n    {\"name\": \"error\", \"type\": \"string\"}\n]\n\n# Clear datasets and set schemas\noutput_data.write_schema(output_schema, drop_and_create\u003dTrue)\nlog_data.write_schema(log_schema, drop_and_create\u003dTrue)\n\n# Get writers for appending\noutput_writer \u003d output_data.get_writer()\nlog_writer \u003d log_data.get_writer()\n\n# Thread-safe variables\nprice_total \u003d 0\nprocessed_count \u003d 0\nwrite_lock \u003d threading.Lock()\nprice_lock \u003d threading.Lock()\n\ndef clean_filename_unicode_safe(filename):\n    \"\"\"Clean filename while preserving Unicode characters\"\"\"\n    # Normalize Unicode characters\n    filename \u003d unicodedata.normalize(\u0027NFC\u0027, filename)\n    # Only replace characters that are problematic for file systems\n    cleaned \u003d re.sub(r\u0027[\u003c\u003e:\"/\\\\|?*\\s]\u0027, \u0027_\u0027, filename)  # Only filesystem-unsafe chars\n    # Remove multiple consecutive underscores\n    cleaned \u003d re.sub(r\u0027_+\u0027, \u0027_\u0027, cleaned)\n    return cleaned\n\ndef process_file(path):\n    \"\"\"Process a single file and immediately write results\"\"\"\n    global price_total, processed_count\n    \n    basename \u003d os.path.basename(path)\n    dirname \u003d os.path.dirname(path)\n    \n    output_row \u003d {\n        \u0027Dir_Name\u0027: dirname,\n        \u0027File_Name\u0027: basename,\n        \u0027Chunk_Id\u0027: 0,\n        \u0027Chunk_Text\u0027: \u0027\u0027,\n        \u0027Chunk_Length\u0027: 0,\n        \u0027Token_Count\u0027: 0\n    }\n    \n    try:\n        file_size \u003d folder.get_path_details(path)[\u0027size\u0027] / (1024 * 1024)\n        memory_usage \u003d resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n    except Exception as e:\n        print(\"Error getting file details for {}: {}\".format(path, e))\n        file_size \u003d 0\n        memory_usage \u003d 0\n    \n    log_row \u003d {\n        \u0027directory\u0027: dirname,\n        \u0027file_name\u0027: basename,\n        \u0027file_size\u0027: \u0027{:.2f} MB\u0027.format(file_size), \n        \u0027memory_total\u0027: \u0027{:.2f} MB\u0027.format(memory_usage),\n        \u0027page_count\u0027: \u0027\u0027, \n        \u0027token_count\u0027: \u0027\u0027, \n        \u0027extract_duration\u0027: \u0027\u0027,\n        \u0027price_total\u0027: \u0027\u0027,\n        \u0027error\u0027: \u0027\u0027\n    }\n    \n    # Use shorter thread ID and Unicode-safe filename cleaning\n    thread_id \u003d threading.current_thread().ident\n    short_thread_id \u003d str(thread_id)[-6:]  # Only last 6 digits\n    \n    # Unicode-safe filename cleaning - preserves Chinese characters\n    clean_basename \u003d clean_filename_unicode_safe(basename)\n    \n    # Create shorter, cleaner filename\n    local_path \u003d os.path.join(folder_pref, \"{}_{}\".format(short_thread_id, clean_basename))\n    \n    pdf_path \u003d None\n    chunks_result \u003d [output_row]\n    price_result \u003d 0\n    \n    try:\n        _, extension \u003d os.path.splitext(path)\n        \n        # Download file - exactly like your old code\n        with folder.get_download_stream(path) as f_in:\n            with open(local_path, \u0027wb\u0027) as f_out:\n                f_out.write(f_in.read())\n        \n        # Process based on file type - old approach + .doc formats\n        if extension \u003d\u003d \u0027.pdf\u0027:   \n            pdf_path \u003d local_path\n\n        elif extension in [\u0027.pptx\u0027, \u0027.xlsx\u0027, \u0027.docx\u0027]:\n            pdf_path \u003d local_path.replace(extension, \u0027.pdf\u0027)\n\n            try:\n                if extension \u003d\u003d \u0027.xlsx\u0027:  # Keep original Excel handling\n                    if file_size \u003e 20:\n                        log_row[\u0027error\u0027] \u003d \u0027File is too large\u0027\n                        with write_lock:\n                            log_writer.write_row_dict(log_row)\n                            output_writer.write_row_dict(output_row)\n                        return\n                    \n                    prepare_excel_for_pdf(local_path)\n                \n                convert_office_to_pdf_optimized(local_path, pdf_path, folder_pref, environ)\n                print(\"File {} successfully converted to pdf\".format(local_path))\n\n            except Exception as e:\n                message \u003d str(e)\n                print(\"Error during file conversion: file: {}, error: {}\".format(local_path, message))\n                \n                log_row[\u0027error\u0027] \u003d \"File conversion issue: \u0027{}\u0027\".format(message)\n                with write_lock:\n                    log_writer.write_row_dict(log_row)\n                    output_writer.write_row_dict(output_row)\n                return\n        else:\n            print(\"Skipping unsupported file type: {}\".format(path))\n            \n            log_row[\u0027error\u0027] \u003d \"Unsupported file type: \u0027{}\u0027\".format(path)\n            with write_lock:\n                log_writer.write_row_dict(log_row)\n                output_writer.write_row_dict(output_row)\n            return\n        \n        # Extract content\n        try:\n            start_time \u003d time()\n            chunks, pages, tokens, price \u003d extract_image_pdf(\n                pdf_path, dirname, basename, openai, model_name, price_per_mil_token\n            )\n            duration \u003d round(time() - start_time)\n            \n            chunks_result \u003d chunks\n            price_result \u003d price\n            \n            print(\"Extracted {} chunks from {} ({} tokens, ${:.2f})\".format(\n                len(chunks), basename, tokens, price))\n            \n            # Update log row with success info\n            log_row.update({\n                \u0027page_count\u0027: str(pages),\n                \u0027token_count\u0027: str(tokens),\n                \u0027extract_duration\u0027: \u0027{}s\u0027.format(duration),\n                \u0027price_total\u0027: \u0027{:.2f}$\u0027.format(price)\n            })\n                \n        except Exception as e:\n            message \u003d str(e)\n            print(\"Error during llm request for the file {}: {}\".format(pdf_path, message))\n            \n            log_row[\u0027error\u0027] \u003d \"LLM request issue: \u0027{}\u0027\".format(message)\n            with write_lock:\n                log_writer.write_row_dict(log_row)\n                output_writer.write_row_dict(output_row)\n            return\n        \n    except Exception as e:\n        log_row[\u0027error\u0027] \u003d \"Unexpected error: {}\".format(str(e))\n        print(\"Unexpected error processing {}: {}\".format(path, e))\n        \n    finally:\n        # Clean up files\n        cleanup_files \u003d []\n        if local_path and os.path.exists(local_path):\n            cleanup_files.append(local_path)\n        if pdf_path and pdf_path !\u003d local_path and os.path.exists(pdf_path):\n            cleanup_files.append(pdf_path)\n            \n        for cleanup_file in cleanup_files:\n            try:\n                os.remove(cleanup_file)\n            except Exception as e:\n                print(\"Error cleaning up file {}: {}\".format(cleanup_file, e))\n    \n    # Write results immediately using the writer (thread-safe)\n    with write_lock:\n        # Write each chunk individually using write_row_dict\n        for chunk in chunks_result:\n            output_writer.write_row_dict(chunk)\n        \n        # Write log entry\n        log_writer.write_row_dict(log_row)\n    \n    # Update global price counter (thread-safe)\n    with price_lock:\n        price_total +\u003d price_result\n        processed_count +\u003d 1\n        \n        # Progress update every 10 files\n        if processed_count % 10 \u003d\u003d 0:\n            print(\"Progress: {}/{} files completed. Total cost: ${:.2f}\".format(\n                processed_count, len(all_paths), price_total))\n\n# Process all files\ntotal_files \u003d len(all_paths)\nprint(\"Starting processing of {} files\".format(total_files))\n\ntry:\n    with ThreadPoolExecutor(max_workers\u003d8) as executor:\n        # Submit all tasks\n        future_to_path \u003d {executor.submit(process_file, path): path for path in all_paths}\n        \n        # Process results as they complete\n        for future in as_completed(future_to_path):\n            path \u003d future_to_path[future]\n            try:\n                future.result()\n            except Exception as e:\n                print(\"Critical error processing {}: {}\".format(path, e))\n                # Still log the error\n                with write_lock:\n                    error_log \u003d {\n                        \u0027directory\u0027: os.path.dirname(path),\n                        \u0027file_name\u0027: os.path.basename(path),\n                        \u0027file_size\u0027: \u0027\u0027, \u0027memory_total\u0027: \u0027\u0027, \u0027page_count\u0027: \u0027\u0027,\n                        \u0027token_count\u0027: \u0027\u0027, \u0027extract_duration\u0027: \u0027\u0027, \u0027price_total\u0027: \u0027\u0027,\n                        \u0027error\u0027: \u0027Critical failure: {}\u0027.format(str(e))\n                    }\n                    log_writer.write_row_dict(error_log)\n\nfinally:\n    # Close the writers\n    output_writer.close()\n    log_writer.close()\n\nprint(\"Total processing completed. Processed {} files. Total price: ${:.2f}\".format(\n    processed_count, price_total))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}