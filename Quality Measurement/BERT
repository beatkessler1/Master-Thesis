# Install dependencies if needed
# !pip install torch transformers datasets --quiet

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# 1. Load dataset (IMDb reviews: train/test)
dataset = load_dataset("imdb")

# 2. Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

dataset_encoded = dataset.map(tokenize, batched=True)
dataset_encoded.set_format("torch", columns=["input_ids", "attention_mask", "label"])

# 3. Load pretrained BERT model for classification
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# 4. Define training arguments
training_args = TrainingArguments(
    output_dir="./bert-imdb",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=1,   # keep 1 for speed; increase if you want better results
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
)

# 5. Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset_encoded["train"].shuffle(seed=42).select(range(2000)),  # smaller subset for demo
    eval_dataset=dataset_encoded["test"].shuffle(seed=42).select(range(500)),
)

# 6. Train the model
trainer.train()

# 7. Evaluate on test set
results = trainer.evaluate()
print("Evaluation:", results)

# 8. Try out the model
sample_text = "The movie was absolutely wonderful, I loved it!"
inputs = tokenizer(sample_text, return_tensors="pt", truncation=True, padding=True, max_length=128)
outputs = model(**inputs)
pred = torch.argmax(outputs.logits).item()
print("Prediction:", "Positive" if pred == 1 else "Negative")
