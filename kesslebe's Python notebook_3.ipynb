{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-genai-bootcamp-env-general_4gb_1cpu",
      "display_name": "Python in general_4GB_1CPU (env genai-bootcamp-env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1758785176553,
    "tags": [],
    "customFields": {},
    "modifiedBy": "kesslebe",
    "creator": "kesslebe"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport openai\nimport logging\nimport dataiku\nimport resource\nimport pandas as pd\n\nfrom time import time\nfrom utils_prompt import prepare_excel_for_pdf, convert_office_to_pdf, extract_image_pdf"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "logging.getLogger(\u0027openai\u0027).setLevel(logging.WARNING)\nlogging.getLogger(\u0027httpx\u0027).setLevel(logging.WARNING)\n\nclient \u003d dataiku.api_client()\nauth_info \u003d client.get_auth_info(with_secrets\u003dTrue)\n\nfor secret in auth_info[\u0027secrets\u0027]:\n    if secret[\u0027key\u0027] \u003d\u003d \u0027neura-gpt-test-key\u0027:\n        API_KEY \u003d secret[\u0027value\u0027]\n        break\n        \nopenai.api_type \u003d \u0027azure\u0027\nopenai.azure_endpoint \u003d \u0027https://iapi-test.merck.com/gpt/libsupport\u0027\nopenai.api_version \u003d \u00272025-03-01-preview\u0027\nopenai.api_key \u003d API_KEY\n\nmodel_name \u003d \u0027gpt-4o-2024-11-20\u0027\nprice_per_mil_token \u003d (2.5, 10)\n\nprint(\"test\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs - MULTI-FOLDER VERSION\nfolder_pref \u003d \u0027/tmp/auto_soe\u0027\n\nenviron \u003d os.environ.copy()\nenviron[\u0027XDG_RUNTIME_DIR\u0027] \u003d folder_pref\n\nos.makedirs(folder_pref, exist_ok\u003dTrue)\n\n# Multi-folder setup\nsources \u003d [\n    dataiku.Folder(\u0027jpYSb1ya\u0027),\n    dataiku.Folder(\u0027Ms4f1qz8\u0027)\n]\n\nall_entries \u003d []\nfor fobj in sources:\n    for p in fobj.list_paths_in_partition():\n        all_entries.append((fobj, p))\nprint(all_entries)  \n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize datasets\noutput_data \u003d dataiku.Dataset(\"MLF_Chunks\")\noutput_df \u003d pd.DataFrame()\n\nlog_data \u003d dataiku.Dataset(\"QDoc_Log\")\nlog_cols \u003d [\u0027directory\u0027, \u0027file_name\u0027, \u0027file_size\u0027, \u0027memory_total\u0027, \u0027page_count\u0027, \u0027token_count\u0027, \u0027extract_duration\u0027, \u0027price_total\u0027, \u0027error\u0027]\n\nlog_df \u003d pd.DataFrame(columns\u003dlog_cols)\nlog_data.write_with_schema(log_df)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "price_total \u003d 0\ntokens_total \u003d 0\noutput_list \u003d []\n\nfor folder, path in all_entries:  # Keep same variable names as original\n    basename \u003d os.path.basename(path)\n    dirname \u003d os.path.dirname(path)\n\n    output_row \u003d {\n        \u0027Dir_Name\u0027: dirname,\n        \u0027File_Name\u0027: basename,\n        \u0027Chunk_Id\u0027: 0,\n        \u0027Chunk_Text\u0027: \u0027\u0027,\n        \u0027Chunk_Length\u0027: 0,\n        \u0027Token_Count\u0027: 0\n    }\n\n    file_size \u003d folder.get_path_details(path)[\u0027size\u0027] / (1024 * 1024)\n    memory_usage \u003d resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n    \n    log_row \u003d {\u0027directory\u0027: dirname,\n               \u0027file_name\u0027: basename,\n               \u0027file_size\u0027:  f\u0027{file_size:.2f} MB\u0027, \n               \u0027memory_total\u0027: f\u0027{memory_usage:.2f} MB\u0027,\n               \u0027page_count\u0027: \u0027\u0027, \n               \u0027token_count\u0027: \u0027\u0027, \n               \u0027extract_duration\u0027: \u0027\u0027,\n               \u0027price_total\u0027: \u0027\u0027,\n               \u0027error\u0027: \u0027\u0027}\n\n    log_df \u003d pd.concat([log_df, pd.DataFrame([log_row])], ignore_index\u003dTrue)\n    log_data.write_with_schema(log_df)\n    \n    # Create per-folder directory to avoid filename collisions\n    src_id \u003d getattr(folder, \"get_id\", lambda: hex(id(folder)))()\n    local_path \u003d os.path.join(folder_pref, str(src_id), basename)\n    os.makedirs(os.path.dirname(local_path), exist_ok\u003dTrue)\n    \n    _, extension \u003d os.path.splitext(path)\n\n    # Use EXACT same download method as working code\n    with folder.get_download_stream(path) as f_in:  # Fixed: use \u0027path\u0027 not \u0027p\u0027\n        with open(local_path, \u0027wb\u0027) as f_out:\n            f_out.write(f_in.read())\n    \n    if extension \u003d\u003d \u0027.pdf\u0027:   \n        pdf_path \u003d local_path\n\n    elif extension in [\u0027.pptx\u0027, \u0027.xlsx\u0027, \u0027.docx\u0027]:\n        pdf_path \u003d local_path.replace(extension, \u0027.pdf\u0027)\n\n        try:\n            if extension \u003d\u003d \u0027.xlsx\u0027:\n                if file_size \u003e 20:\n                    log_df.at[log_df.index[-1], \u0027error\u0027] \u003d \u0027File is too large\u0027\n                    log_data.write_with_schema(log_df)\n\n                    output_list.append(output_row)\n                    continue\n                \n                prepare_excel_for_pdf(local_path)\n            \n            convert_office_to_pdf(local_path, pdf_path, folder_pref, environ)\n            print(f\"File {local_path} successfully converted to pdf\")\n\n        except Exception as e:\n            message \u003d str(e)\n            print(f\"Error during file conversion: file: {local_path}, error: {message}\")\n            \n            log_df.at[log_df.index[-1], \u0027error\u0027] \u003d f\"File conversion issue: \u0027{message}\u0027\"\n            log_data.write_with_schema(log_df)\n            \n            output_list.append(output_row)\n            continue\n    else:\n        print(f\"Skipping unsupported file type: {path}\")\n        \n        log_df.at[log_df.index[-1], \u0027error\u0027] \u003d f\"Unsupported file type: \u0027{path}\u0027\"\n        log_data.write_with_schema(log_df)\n        \n        output_list.append(output_row)\n        continue\n        \n    try:\n        start_time \u003d time()\n        chunks, pages, tokens, price \u003d extract_image_pdf(pdf_path, dirname, basename, openai, model_name, price_per_mil_token)\n        duration \u003d round(time() - start_time)\n        \n        os.remove(pdf_path)\n        print(f\"The content from the file {pdf_path} successfully extracted.\")\n            \n    except Exception as e:\n        message \u003d str(e)\n        print(f\"Error during llm request for the file {pdf_path}: {message}\")\n\n        log_df.at[log_df.index[-1], \u0027error\u0027] \u003d f\"LLM request issue: \u0027{message}\u0027\"\n        log_data.write_with_schema(log_df)\n        \n        output_list.append(output_row)\n        continue\n    \n    output_list.extend(chunks)\n    price_total +\u003d price\n    \n    print(f\"Tokens: {tokens}, Duration: {duration}s, Price: {price_total:.2f}$\")\n    \n    log_df.loc[log_df.index[-1], [\u0027page_count\u0027, \u0027token_count\u0027, \u0027extract_duration\u0027, \u0027price_total\u0027]] \u003d [f\u0027{pages}\u0027, f\u0027{tokens}\u0027, f\u0027{duration}s\u0027, f\u0027{price_total:.2f}$\u0027]\n    log_data.write_with_schema(log_df)\n    \n    if os.path.exists(local_path):\n        os.remove(local_path)\n\n# Write recipe outputs\noutput_df \u003d pd.concat([output_df, pd.DataFrame(output_list)], ignore_index\u003dTrue)\noutput_data.write_with_schema(output_df)\n\nprint(f\"Processing complete! {len(output_list)} chunks from {len(all_entries)} files, Total cost: ${price_total:.2f}\")\n"
      ],
      "outputs": []
    }
  ]
}